# Objetivo  
O objetivo desta POC (Prova de conceito) e montar uma plataforma de dados   m√≠nima usando ferramentas de c√≥digo aberta. 

# Objetivos espec√≠ficos  
- Montar uma plataforma de dados m√≠nima com as camadas de orquestra√ß√£o, processamento e armazenamento 
- Usar ferramentas de c√≥digo aberto 

# Ferramentas usadas  
- [airflow - 3.0.2](https://airflow.apache.org/docs/apache-airflow/stable/index.html)  
- [pyspark - 4.0.0](https://spark.apache.org/docs/latest/api/python/index.html#)  
- [delta lake - 4.0.0](https://docs.delta.io/latest/index.html)
- [minio - RELEASE.2025-04-22T22-12-26Z](https://min.io/docs/minio/kubernetes/upstream/index.html)
- [papermil - 2.4.0](https://papermill.readthedocs.io/en/latest/index.html)  

# Configura√ß√£o dos servi√ßos  
> Aten√ß√£o! Este projeto deve ser executado em uma maquina Linux ou wsl2 do windows  
> 
> Para saber mais sobre wsl2 click [aqui](https://learn.microsoft.com/pt-br/windows/wsl/about)  

## 0 - Pr√©-requisitos    
- Certifique-se de ter o Docker e Docker Compse instalados na sua m√°quina.    
- Para instalar o Docker, click [aqui](https://docs.docker.com/engine/install/)   
- Para instalar o Docker Compose, clique [aqui](https://docs.docker.com/compose/install/)    

## 1 - Abra o terminal de sua prefer√™ncia  
- No Windows, click [aqui](https://elsefix.com/pt/tech/tejana/how-to-use-windows-terminal-in-windows-10-beginners-guide) para saber como abrir um terminal.  
- No Linux, click [aqui](https://pt.linux-console.net/?p=12268) para saber como abrir um terminal.  

## 2 - Clone do repositorio  
- Click [aqui](https://docs.github.com/pt/repositories/creating-and-managing-repositories/cloning-a-repository) e siga o passo de como clonar um resposit√≥rio do github  

## 3 - Mude para a pasta do projeto  
~~~bash  
$ cd projeto-lakehouse  
~~~  

## 4 - Configure uma rede externa do docker  
~~~bash  
$ docker network create projeto-lakehouse  
~~~  

## 5 - Crie o arquivo .env baseado no .env.template  
~~~bash  
$ cp .env.template .env  
~~~  

## 6 - Abra um editor de c√≥digo de sua prefer√™ncia.  
> Sugest√£o! Use o vscode.  
- No Windows com wsl2, click [aqui](https://learn.microsoft.com/pt-br/windows/wsl/tutorials/wsl-vscode) para saber como instalar e usar.  
- No Linux, click [aqui](https://code.visualstudio.com/docs/setup/linux) para saber como instalar e usar.  

## 7 - Abra o arquivo .env e preencha os valores das vari√°veis  

## 8 - Volte para o terminal e execute o comando abaixo para iniciar os servi√ßos  
~~~bash  
$ docker compose up  
~~~  
Este comando deixa o terminal preso, se preferir pode usar o comando abaixo para iniciar os servi√ßos e liberar o terminal  
~~~bash  
$ docker compose up -d  
~~~  
  
## 9 - Verifique o status dos servi√ßos Docker  
~~~bash  

$ docker ps

~~~  
Voc√™ dever√° ver os cont√™ineres para Airflow (database, scheduler, worker), MinIO, Postgres, etc., com o status Up.  

Para verificar os logs de um servi√ßo espec√≠fico (substitua airflow_api_server pelo nome do servi√ßo que deseja inspecionar, como minio, airflow_scheduler etc.):  

~~~bash  

$ docker compose logs -f airflow_dag-processor

~~~  
Pressione Ctrl + C para sair dos logs.  

## 10 - Acesse as interfaces dos servi√ßos  

  Acesse o MinIO pela interface web em:  

~~~bash 

http://localhost:10000  

~~~ 

Conforme definido no docker-compose.yml, certifique-se de que essa porta foi mapeada corretamente no seu ambiente.  

Fa√ßa login utilizando as credenciais definidas no arquivo .env:  

Access Key: MINIO_ROOT_USER  

Key: MINIO_ROOT_PASSWORD  

---
Para a cria√ß√£o dos buckets, ap√≥s o login, siga os passos abaixo que representam as camadas da arquitetura de medalh√£o: 

No menu lateral esquerdo, clique em Administrator. 

Em seguida, clique em Buckets. 

No canto superior direito da tela, clique em Create Bucket. 

Crie o bucket inicial chamado landing e clique em Create Bucket para confirmar. 

Para verificar a cria√ß√£o, acesse o menu Object Browser (no menu esquerdo) e confira se o bucket landing aparece na lista. 

Repita o processo para criar os demais buckets necess√°rios, como: bronze, silver, gold, entre outros. 

Esses buckets ser√£o usados para armazenar os dados nos diferentes est√°gios de processamento do pipeline. 


## 11 - Configura√ß√£o da conex√£o do MinIO 

Para permitir a comunica√ß√£o entre o pipeline e o MinIO, foi necess√°rio configurar uma conex√£o chamada minio_connection. 

Essa configura√ß√£o foi realizada dentro do notebook, por meio da cria√ß√£o de um arquivo .json com as credenciais e o endpoint de acesso. O arquivo foi salvo no seguinte caminho: 

src/dags/geography/coordinates/variables/minio_connection.json 
O conte√∫do do arquivo inclui as seguintes informa√ß√µes: 

"endpoint": URL interna de acesso ao MinIO 

"access_key": chave de acesso (gerada pelo MinIO)

"key": chave secreta (tamb√©m gerada pelo MinIO)

Como gerar as credenciais: 

Acesse o MinIO pela interface web (http://localhost:10000).

No menu lateral, clique em Access Keys.

Clique em Create Access Key.

Copie os valores de Access Key e Secret Key gerados.

Preencha o arquivo minio_connection.json com essas informa√ß√µes.

Esse arquivo ser√° utilizado posteriormente para autentica√ß√£o autom√°tica nas DAGs ou scripts que interagem com o armazenamento do MinIO.

## 12 - Configura√ß√£o do ambiente de desenvolvimento

Ambiente Python com uv, clique [aqui](https://docs.astral.sh/uv/) para saber como instalar e usar.

Ap√≥s configurar os servi√ßos, o ambiente Python foi instalado e gerenciado com a ferramenta uv, que oferece uma forma r√°pida e moderna de instalar depend√™ncias Python. 

O terminal foi aberto na pasta raiz do projeto dentro do VSCode, onde o comando de instala√ß√£o das depend√™ncias foi executado (com base no arquivo pyproject.toml).

~~~bash 

uv sync 

~~~ 

O teste de funcionamento foi bem-sucedido. A aplica√ß√£o Spark processou os dados conforme o esperado e os arquivos foram salvos corretamente no MinIO no formato Delta Lake. Isso valida a capacidade do ambiente de:

Processar dados com Apache Spark.

Utilizar as capacidades transacionais e de versionamento do Delta Lake.

Persistir dados de forma eficiente em um armazenamento de objetos compat√≠vel com S3 (MinIO).

A arquitetura demonstrada aqui estabelece uma base s√≥lida para pipelines de dados escal√°veis e confi√°veis, utilizando tecnologias de c√≥digo aberto para gerenciamento de data lakes.

## 13 Validando de infraestrura: Spark, Delta Lake e MinIO para um Data Lake 

Este reposit√≥rio documenta um teste de valida√ß√£o da infraestrutura de dados, executado localmente no VS Code. O objetivo foi verificar se a escrita de dados no formato Delta Lake para um bucket MinIO (rodando via Docker) funcionava corretamente utilizando o Apache Spark com PySpark. 

O que foi testado: 

Execu√ß√£o local do c√≥digo PySpark no VS Code, arquivo : src_lnd_geography_coordinates.ipynb 

Grava√ß√£o de arquivos Delta (.parquet) em um bucket MinIO (S3 compat√≠vel) 

Integra√ß√£o com bibliotecas essenciais: delta-spark, hadoop-aws, aws-java-sdk 

Persist√™ncia bem-sucedida dos dados, com estrutura Delta armazenada no MinIO 

Resultado: A escrita no formato Delta funcionou conforme esperado, validando a comunica√ß√£o entre Spark e MinIO e confirmando que a infraestrutura est√° pronta para os pr√≥ximos passos do pipeline. 


## 14 - Ativa√ß√£o do Apache Airflow e Configura√ß√£o da DAG

Para resolver o erro de conex√£o da DAG `geography_coordinates` no Airflow, foram realizados os seguintes passos:

- O Airflow foi aberto no navegador na porta **8080** (ex: http://localhost:8080). 

- No menu superior, acessei **Admin > Variables**. 

- Cliquei em **Add Variable** no canto superior direito. 

---

- Criei uma vari√°vel com: 

  - **Key:** `minio_connection.json` (nome do arquivo JSON localizado na pasta `variables` do projeto) 

  - **Value:** o conte√∫do completo do arquivo JSON copiado e colado diretamente. 

- Ap√≥s essa configura√ß√£o, o erro da DAG desapareceu. 

- A DAG `geography_coordinates` aparece ativa no menu lateral esquerdo do Airflow, indicando conex√£o bem-sucedida com MinIO, PySpark e Delta Lake.

Este procedimento validou a integra√ß√£o do Airflow com a infraestrutura de dados local, garantindo a execu√ß√£o confi√°vel das pipelines.

## 15 Cria√ß√£o da DAG ibge_pme e Organiza√ß√£o do Fluxo de Diret√≥rios 

Para estruturar o pipeline de ingest√£o e transforma√ß√£o dos dados da Pesquisa Mensal de Emprego (PME/IBGE), foi criada a DAG ibge_ipca_amplo no Airflow, com execu√ß√£o agendada a cada 3 minutos (schedule="*/3 * * * *"). A DAG est√° localizada no caminho: 

~~~bash 

/projeto-lakehouse/src/dags/ibge/ipca_amplo/ibge_ipca_amplo.py 

~~~ 

Essa DAG executa um fluxo dividido nas seguintes camadas: 

- Landing: ingest√£o inicial dos dados 

- Bronze: padroniza√ß√£o e limpeza 

- Silver: transforma√ß√£o e enriquecimento 

Cada camada possui um notebook .ipynb respons√°vel por sua respectiva etapa de processamento.  

Vazias por enquanto.
Cada notebook √© executado por um PapermillOperator. 

### L√≥gica de Depend√™ncia entre Tarefas

A DAG define a seguinte sequ√™ncia entre as tasks: 

~~~bash 

landing ‚Üí bronze ‚Üí silver

~~~ 

Essa DAG utiliza notebooks que acessam o MinIO por meio da vari√°vel minio_connection, configurada previamente no Airflow (conforme explicado na se√ß√£o anterior).

## 16 Detalhamento T√©cnico da Integra√ß√£o: Ingest√£o de Dados e Execu√ß√£o com Airflow

Ap√≥s a cria√ß√£o da DAG `ibge_ipca_amplo` e a organiza√ß√£o dos notebooks por camadas (Landing, Bronze, Silver), foram implementadas as seguintes etapas para viabilizar a ingest√£o de dados e execu√ß√£o das tarefas via Airflow:

---

### 1. Desenvolvimento do Processo de Ingest√£o de Dados

Foi desenvolvido um fluxo de ingest√£o baseado no arquivo `ibge_ipca_amplo.py`. Nele, foi criada a fun√ß√£o `ibge_ipca_amplo()` que realiza as seguintes a√ß√µes:

- Define o caminho para os dados de entrada na camada **Landing**.
- Cria a conex√£o com o **MinIO** utilizando as configura√ß√µes passadas via vari√°vel `minio_connection`.
- Define a l√≥gica para ler os arquivos `.json` com **listas aninhadas**, desaninh√°-las e transformar os dados em formato tabular.
- Salva os dados tratados no Data Lake no formato de **tabela Delta**.

---

Essa etapa √© executada por meio do notebook `src_lnd_ibge_ipca_amplo.ipynb`, que possui o seguinte fluxo:

1. **Importa√ß√£o de bibliotecas**.
2. **Cria√ß√£o da conex√£o com o MinIO** via PySpark.
3. **Cria√ß√£o do SparkSession**.
4. **Leitura e transforma√ß√£o dos arquivos JSON com listas aninhadas**.
5. **Grava√ß√£o no MinIO** no formato Delta.
6. **Configura√ß√£o b√°sica de logging** para monitoramento.

### 2. Visualiza√ß√£o dos Dados no MinIO

Com os dados processados, √© poss√≠vel visualizar os arquivos diretamente no navegador:

- Acesse o MinIO via browser.
- Clique em **"Buckets"** ‚Üí Selecione o bucket da camada `landing`.
- O lado esquerdo mostrar√° a **quantidade de objetos e uso de espa√ßo**.
- Clique em **"Object Browser"** para navegar entre pastas e localizar os arquivos armazenados.

### 3. Corre√ß√µes Necess√°rias para Execu√ß√£o via Airflow

Durante os testes iniciais de execu√ß√£o da DAG via Airflow, foram encontrados alguns problemas que exigiram a√ß√µes manuais para garantir o funcionamento correto. Abaixo est√£o os passos realizados:

#### Etapa 1 ‚Äì Cria√ß√£o do Diret√≥rio de Logs no Container

Para que o Papermill possa salvar os notebooks de sa√≠da com os par√¢metros executados, √© necess√°rio que o diret√≥rio de destino exista. Isso foi feito manualmente dentro do container do `airflow-worker`:

~~~bash
docker exec -it airflow-worker bash
mkdir -p /opt/airflow/logs/tasks/landing
~~~

> Esse caminho √© utilizado automaticamente pelo PapermillOperator para armazenar os notebooks executados com par√¢metros.

#### Etapa 2 ‚Äì Adi√ß√£o da Tag `parameters` no Notebook

O Papermill precisa saber quais c√©lulas cont√™m par√¢metros que devem ser substitu√≠dos em tempo de execu√ß√£o. Para isso, a c√©lula onde est√° a vari√°vel `minio_connection = ""` precisa ser marcada com a tag `"parameters"`.

**Passos para adicionar a tag no VS Code:**

1. Abra o notebook `src_lnd_ibge_ipca_amplo.ipynb`.
2. Localize a c√©lula onde est√° `minio_connection = ""` (logo ap√≥s as importa√ß√µes).
3. Clique com o bot√£o direito sobre essa c√©lula e selecione **"Edit Cell Tags"**.
4. Insira a seguinte tag:

~~~json
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7681ecd0",
   "metadata": {
   "tags": [
  "parameters"
 ]  
   },
   "outputs": [],
   "source": [
    "minio_connection = \"\""
   ]
  },
~~~

5. Salve o notebook.

> Isso garante que o Airflow consiga injetar corretamente os par√¢metros definidos no PapermillOperator ao executar o notebook.

### 4. Testes e Valida√ß√£o no Airflow

Com os ajustes feitos:

- A pasta de logs criada manualmente.
- A c√©lula marcada com a tag `parameters`.

Foi poss√≠vel executar a DAG no Airflow com sucesso. O notebook √© processado, os dados s√£o salvos corretamente no MinIO, e o output do notebook √© armazenado em:

~~~bash
/opt/airflow/logs/tasks/landing/
~~~

Estrutura de Diret√≥rios e Arquivos:  
Abaixo est√° a hierarquia de pastas utilizada para organizar os notebooks da DAG ibge_pme: 

~~~bash 

ibge/
‚îî‚îÄ‚îÄ ipca_amplo/
    ‚îú‚îÄ‚îÄ ibge_ipca_amplo.py
    ‚îú‚îÄ‚îÄ tasks/
    ‚îÇ   ‚îú‚îÄ‚îÄ landing/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ artifacts/
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ src_lnd_ibge_ipca_amplo.ipynb
    ‚îÇ   ‚îú‚îÄ‚îÄ bronze/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lnd_brz_ipca_amplo.ipynb
    ‚îÇ   ‚îî‚îÄ‚îÄ silver/
    ‚îÇ       ‚îî‚îÄ‚îÄ brz_slv_ipca_amplo.ipynb
    ‚îî‚îÄ‚îÄ variables/
        ‚îî‚îÄ‚îÄ minio_connection.json
~~~ 

Cada notebook √© executado por um PapermillOperator. 

### 5. Automa√ß√£o da Coleta de Dados IBGE (API SIDRA)

Antes, foi realizado um teste manual para a coleta dos dados, definindo per√≠odos espec√≠ficos e fazendo m√∫ltiplas requisi√ß√µes para obter todos os arquivos desejados, no entanto, devido a quantidade de arquivos essa pr√°tica √© invi√°vel, especialmente devido √† limita√ß√£o de volume de dados que podem ser requisitados por vez na API do IBGE.

Para resolver esse problema, foi implementada uma automatiza√ß√£o do processo de requisi√ß√£o, que agora gera de forma din√¢mica todos os per√≠odos do tipo YYYYMM desde janeiro de 2020 at√© a data atual. Com isso, a URL de consulta √† API √© montada automaticamente para cada per√≠odo, garantindo que os dados sejam baixados de forma cont√≠nua, escal√°vel e organizada. 

Essa automatiza√ß√£o foi feita apenas no par√¢metro periodos, onde o mesmo foi colocado em outra celula e com o uso da biblioteca datetime foi criado essa requisi√ß√£o m√™s a m√™s. 

Para garantir uma coleta est√°vel e eficiente dos dados, foi adicionado um intervalo de 0.5 segundos entre cada requisi√ß√£o √† API. Essa pausa tem dois objetivos principais: Evitar sobrecarga na API p√∫blica do IBGE e evitar sobrecarga no hardware local. 

# 17 DAG: Levantamento de Pre√ßos ANP (Camada Landing)

## Descri√ß√£o do Projeto

Este projeto automatiza a extra√ß√£o, download e armazenamento de arquivos p√∫blicos da Ag√™ncia Nacional do Petr√≥leo (ANP) contendo s√©ries hist√≥ricas do levantamento de pre√ßos de combust√≠veis no Brasil.

Fonte dos Dados: 

https://www.gov.br/anp/pt-br/assuntos/precos-e-defesa-da-concorrencia/precos/precos-revenda-e-de-distribuicao-combustiveis/informacoes-levantamento-de-precos-de-combustiveis

A DAG (Directed Acyclic Graph) implementada foi desenvolvida para ser executada no Apache Airflow, fazendo a orquestra√ß√£o do processo de coleta dos dados, armazenamento no datalake (MinIO) e garantindo a organiza√ß√£o dos dados na camada **landing**.

---

## Estrutura de Pastas e Arquivos

src/dags/
‚îî‚îÄ‚îÄ anp/serie_levantamento_precos/
‚îú‚îÄ‚îÄ tasks/
‚îÇ ‚îú‚îÄ‚îÄ bronze/
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ lnd_brz_anp_serie_levantamento_precos.ipynb # Notebook para camada bronze
‚îÇ ‚îî‚îÄ‚îÄ landing/
‚îÇ ‚îú‚îÄ‚îÄ artifacts/ # Pasta para artefatos gerados no processo (logs, dados auxiliares)
‚îÇ ‚îú‚îÄ‚îÄ download/ # Pasta tempor√°ria onde arquivos baixados s√£o armazenados localmente
‚îÇ ‚îî‚îÄ‚îÄ src_lnd_anp_serie_levantamento_precos.ipynb # Notebook principal que executa a extra√ß√£o, download e upload para MinIO
‚îú‚îÄ‚îÄ variables/
‚îÇ ‚îî‚îÄ‚îÄ minio_connection.json # Arquivo com credenciais de acesso ao MinIO
‚îî‚îÄ‚îÄ anp_serie_levantamento_precos.py # Script Python para execu√ß√£o da DAG no Airflow


### 1. Extra√ß√£o de URLs de arquivos

A DAG inicia acessando a p√°gina p√∫blica da ANP onde est√£o dispon√≠veis os arquivos de s√©ries hist√≥ricas de pre√ßos.

Utilizamos um crawler simples implementado com a biblioteca **BeautifulSoup** para fazer o parsing da p√°gina HTML e extrair os links que cont√™m arquivos `.xls`, `.xlsx` e `.zip` que contenham no nome a palavra "mensal, s√≥ foram encontrados arquivos '.xlsx'.

Isso garante que apenas arquivos relevantes para o levantamento mensal sejam coletados.

### 2. Download dos arquivos

Com a lista de URLs em m√£os, a DAG baixa os arquivos para uma pasta local tempor√°ria chamada `download`.

Cada arquivo √© salvo localmente com seu nome original.

Durante o download, metadados s√£o coletados, como nome do arquivo, link de origem, caminho local e data/hora do download.

H√° um pequeno delay entre downloads para evitar sobrecarga no servidor da ANP.

### 3. Upload para o datalake (MinIO)

Ap√≥s o download, os arquivos s√£o enviados para o bucket `landing` dentro do datalake MinIO.

O caminho dentro do bucket segue o prefixo `anp/serie_levantamento_precos/` para organiza√ß√£o.

O MinIO √© uma solu√ß√£o de armazenamento compat√≠vel com S3 que permite armazenar os arquivos de forma segura e acess√≠vel para processos posteriores.

O upload √© realizado via SDK MinIO em Python, utilizando as credenciais armazenadas em `variables/minio_connection.json`.

### 4. Limpeza

Depois que os arquivos s√£o carregados no datalake, os arquivos tempor√°rios locais na pasta `download` s√£o exclu√≠dos para economizar espa√ßo.

---

## Motiva√ß√£o do Fluxo

- **Automa√ß√£o:** Garante que a coleta dos dados da ANP seja feita automaticamente e regularmente.  
- **Organiza√ß√£o:** Arquivos s√£o versionados e organizados em um datalake, facilitando o acesso e futuras etapas de processamento.  
- **Reprodutibilidade:** A DAG pode ser agendada e reexecutada pelo Airflow, mantendo o pipeline confi√°vel e rastre√°vel.  
- **Manuten√ß√£o facilitada:** O c√≥digo est√° organizado em notebooks e scripts para f√°cil manuten√ß√£o e evolu√ß√£o.

---

## 18 Convers√£o de arquivos .xlsx da camada Landing para Bronze(formato Delta)

- Este pipeline tem como objetivo processar os arquivos `.xlsx` provenientes da camada **Landing**, realizar transforma√ß√µes padronizadas nos dados e armazen√°-los na camada **Bronze** no formato Delta Lake. Essa etapa √© crucial para garantir a persist√™ncia confi√°vel, leitura otimizada e versionamento dos dados brutos j√° padronizados, preservando sua integridade para uso nas pr√≥ximas camadas (Silver e Gold).

---
### Estrutura de Entrada

Os arquivos de entrada est√£o localizados no diret√≥rio:

Origem (Landing): Arquivos .xlsx no bucket 
s3a://landing/anp/serie_levantamento_precos/

### Etapas do processo

- Inicializa√ß√£o da SparkSession com suporte a Excel
- Listagem dos arquivos da camada Landing

---
### Etapas Realizadas no Pipeline

1. Listagem dos Arquivos Fonte

- Fun√ß√£o: listar_arquivos_xlsx()

- Descri√ß√£o: Conecta-se ao bucket S3 Landing, filtra arquivos com extens√£o .xlsx usando o prefixo anp/serie_levantamento_precos/.

- Finalidade: Automatiza a detec√ß√£o dos arquivos que devem ser processados.

2. Leitura dos Arquivos Excel com Layout Espec√≠fico

- Fun√ß√£o: ler_arquivos_anp()

- Descri√ß√£o:

- Os arquivos possuem o cabe√ßalho a partir da linha de √≠ndice 11, por isso usamos o par√¢metro header=11.

- Existem rodap√©s n√£o estruturados que s√£o ignorados com skipfooter=12.

- Algumas colunas extras sem nome s√£o eliminadas com df.dropna(axis=1, how='all').

- A leitura √© feita via pandas.read_excel() a partir do bin√°rio do objeto S3.

-Biblioteca usada:

io: para manipular os bin√°rios dos arquivos no formato BytesIO.

3. Convers√£o de DataFrame Pandas para Spark

Feita com:

df_spark = spark.createDataFrame(df_pandas)

Necess√°ria para que os dados possam ser processados em escala distribu√≠da e salvos como Delta.

4. Padroniza√ß√£o dos Nomes das Colunas

- Fun√ß√£o: limpar_nome_colunas_spark()

- O que faz:

- Remove acentos das colunas usando unicodedata.normalize().

- Substitui caracteres inv√°lidos por - usando re.sub().

- Converte todos os nomes para min√∫sculo.

- Evita m√∫ltiplos underscores consecutivos.

- Bibliotecas usadas:

unicodedata: remove acentos (ex: "Pre√ßo" ‚Üí "Preco").

re: express√µes regulares, ou seja, s√£o sequ√™ncias de caracteres que formam um padr√£o.

5. Enriquecimento do DataFrame com Metadados

- Fun√ß√£o: salvar_como_delta()

- Colunas adicionadas:

_carga_data: marca√ß√£o de data/hora da carga (current_timestamp()).

_arquivo_origem: nome original do arquivo .xlsx, com lit(nome_arquivo).

### Estrutura de sa√≠da:

Os arquivos de sa√≠da est√£o localizados no diret√≥rio:

Destino (Bronze): Tabelas Delta salvas em 
s3a://bronze/anp/precos-combustiveis/

---

## 19 DAG: Cadastro Nacional de Obras (CNO)

Este pipeline foi desenvolvido para automatizar o processo de ingest√£o, transforma√ß√£o e armazenamento de dados do **Cadastro Nacional de Obras (CNO)** disponibilizado pela Receita Federal, seguindo a arquitetura de dados em camadas (Landing ‚Üí Bronze).

---

### **1. Cria√ß√£o da DAG e Estrutura de Pastas**

Inicialmente, foi criada a **DAG `cno`** no Apache Airflow para orquestrar todo o fluxo de ingest√£o e transforma√ß√£o.  
A seguir, foi definida a estrutura de diret√≥rios e arquivos dentro do reposit√≥rio, organizada por camadas (`landing`, `bronze`), al√©m de pastas para scripts (`tasks`), vari√°veis e artefatos.

---

### **2. Conex√£o com o MinIO**

O MinIO foi utilizado como **Data Lake** local para armazenamento dos arquivos.  
A configura√ß√£o de acesso ao MinIO foi centralizada em um arquivo `minio_connection.json`, contendo:

- **endpoint** do servidor MinIO (com suporte a HTTP/HTTPS);
- **access_key** e **key** para autentica√ß√£o;
- Nome padr√£o do bucket de destino.

Essas informa√ß√µes s√£o lidas e utilizadas para instanciar o cliente MinIO (`s3_client`) no c√≥digo Python, garantindo uma conex√£o segura e reutiliz√°vel em todo o pipeline.

---

### **3. Ingest√£o de Dados ‚Äî Camada Landing**

Nesta etapa, foi implementada a fun√ß√£o `baixar_e_subir_zip_para_minio` para automatizar o processo de ingest√£o:

1. **Download do arquivo ZIP**  
   - A fun√ß√£o acessa a URL oficial da Receita Federal:  
     ```
     https://arquivos.receitafederal.gov.br/dados/cno/cno.zip
     ```
   - Faz o download completo do arquivo para a mem√≥ria usando `requests`.

2. **Leitura e Extra√ß√£o dos arquivos internos**  
   - O conte√∫do do `.zip` √© carregado em mem√≥ria via `io.BytesIO` (sem necessidade de salvar localmente).
   - O m√≥dulo `zipfile` √© usado para iterar sobre todos os arquivos contidos no ZIP.

3. **Upload para o MinIO (Camada Landing)**  
   - Cada arquivo extra√≠do √© enviado individualmente para o bucket `landing`, preservando o nome original.  
   - A estrutura de destino segue o padr√£o:
     ```
     landing/cno/cno/<nome_do_arquivo>.csv
     ```
   - O m√©todo `s3_client.put_object` √© utilizado para realizar o upload diretamente da mem√≥ria para o MinIO.

4. **Log e Monitoramento**  
   - O processo √© registrado com `logging.info` para acompanhar o progresso, arquivos processados e eventuais erros.

> **Resultado**: todos os arquivos CSV originais do CNO ficam dispon√≠veis na **camada landing** do Data Lake, prontos para serem tratados.

---

## 20 Processamento e Transforma√ß√£o ‚Äî Camada Bronze

Ap√≥s a ingest√£o, cada arquivo CSV da camada landing passa por padroniza√ß√£o e √© convertido para o formato **Delta Lake**.

O processamento de cada arquivo foi feito em **notebooks separados**, um para cada tipo de dado do CNO, por exemplo:
- `cno_cnaes`
- `cno_totais`
- `cno_vinculos`
- etc.

### Etapas de transforma√ß√£o:

1. **Leitura do arquivo CSV**  
   - Utiliza o Spark para ler o arquivo diretamente do MinIO via `s3a://`, preservando o cabe√ßalho (`header=True`) e definindo a codifica√ß√£o correta (`ISO-8859-1` ou `latin1`).

2. **Padroniza√ß√£o dos nomes das colunas**  
   - Fun√ß√£o `remover_acentos`: remove todos os acentos, mantendo a letra original.  
   - Fun√ß√£o `padronizar_nome_coluna`:  
     - Remove acentos  
     - Converte para min√∫sculas  
     - Remove caracteres especiais  
     - Substitui espa√ßos por `_`
   - Fun√ß√£o `padronizar_colunas`: aplica o processo acima em todas as colunas do DataFrame.

3. **Escrita em formato Delta Lake**  
   - Os dados transformados s√£o gravados na camada bronze usando:
     ```python
     df.write.format("delta")
       .mode("overwrite")
       .option("overwriteSchema", "true")
       .save(output_path)
     ```
   - O caminho de destino segue o padr√£o:
     ```
     bronze/cno/cno/<nome_tabela>_delta/
     ```

4. **Valida√ß√£o**  
   - Contagem de linhas (`df.count()`) para garantir que os dados foram carregados corretamente.  
   - Visualiza√ß√£o de amostra (`df.show(20, truncate=False)`) para inspe√ß√£o.

> **Resultado**: arquivos CSV brutos da camada landing s√£o transformados em tabelas Delta na camada bronze, com colunas padronizadas e dados prontos para serem refinados na camada silver.

---

## 21 DAG: Cadastro Nacional de Pessoa Juridica (CNPJ)

Este pipeline foi desenvolvido para automatizar o processo de ingest√£o, transforma√ß√£o e armazenamento de dados do **Cadastro Nacional de Obras (CNPJ)** disponibilizado pela Receita Federal, seguindo a arquitetura de dados em camadas (Landing ‚Üí Bronze).

---

Dentro da DAG tem 10 tasks:

    rbf_cnpj_cnaes; 
    rbf_cnpj_empresas; 
    rbf_cnpj_estabelecimentos; 
    rbf_cnpj_motivos; 
    rbf_cnpj_municipios
    rbf_cnpj_naturezas; 
    rbf_cnpj_paises; 
    rbf_cnpj_qualificacoes; 
    rbf_cnpj_simples; 
    rbf_cnpj_socios

### Etapas do pipeline (camada landing)

1. **Listagem de diret√≥rios existentes**  
   - Consulta o site da Receita Federal para identificar quais diret√≥rios no formato `YYYY-MM` est√£o dispon√≠veis, garantindo que apenas meses com arquivos publicados sejam processados.

2. **Download dos arquivos**  
   - Realizado de forma ass√≠ncrona usando `aiohttp` e um limite de concorr√™ncia (`asyncio.Semaphore`) para n√£o sobrecarregar o servidor.  
   - Valida status HTTP, tipo de conte√∫do e tamanho do arquivo antes de salvar. 
   - Tentativas de download s√£o repetidas at√© 3 vezes em caso de falha.

3. **Extra√ß√£o dos arquivos ZIP**  
   - Cada arquivo baixado √© extra√≠do na pasta `download/` localmente.  
   - Erros de arquivos corrompidos s√£o registrados no log.

4. **Filtragem e leitura dos arquivos CNAE**  
   - Apenas arquivos com nomes correspondentes a task que est√° executada s√£o processados.  
   - Cada CSV √© lido usando `pandas` (`sep=";"`, `encoding="latin1"`, `low_memory=False`) e consolidado em um √∫nico `DataFrame`.

5. **Upload para o MinIO (datalake)**  
   - Os arquivos CSV consolidados s√£o enviados para o bucket `landing` do MinIO, na estrutura `rfb/cnpj_task_correspondente/`.  
   - Mant√©m o caminho relativo para facilitar rastreabilidade.

- A pasta `download/` √© criada automaticamente se n√£o existir.
- Todo o processo √© registrado em logs, permitindo monitoramento.
- Ao final do envio para o data lake a pasta `download/` √© exclu√≠da automaticamente.
- a DAG do Airflow que dispara esses pipelines j√° est√° configurada para execuss√£o.
- Vale ressaltar que no processo de cria√ß√£o das pastas, dentro da camada landing √© preciso ter a pasta variav√©is com o arquivo minio.json dentro com as configura√ß√µes de endpoint;  access_key e key para a conex√£o com o minio.

---

## 22 Processamento dos arquivos da camada landing para bronze

O objetivo do c√≥digo foi:

- Criar um notebook para cada task;
- Processar as informa√ß√µes e salvar em pastas por tipo, assim como foi feito da camada landing.

### 1. Fun√ß√µes de limpeza de texto
- **`remover_acentos`**:  
  - Separa acentos das letras usando `unicodedata.normalize`.
  - Remove os caracteres da categoria `Mn` (marca de acentua√ß√£o).
  - Resultado: transforma `"Jo√£o"` em `"Joao"`.

  - **`limpar_texto`**:  
  - Faz o mesmo processo de remo√ß√£o de acentos.
  - Converte tudo para **min√∫sculo**.
  - Remove espa√ßos extras no in√≠cio e fim.
  - Remove aspas `"` da string.  
  - Exemplo: `" Jo√£o "` ‚Üí `"joao"`.

  ### 2. Leitura do arquivo CSV
- O Spark l√™ o arquivo usando as op√ß√µes:
  - `delimiter=";"` ‚Üí separador ponto e v√≠rgula.
  - `encoding="ISO-8859-1"` ‚Üí garante leitura de caracteres especiais.
  - `header="false"` ‚Üí o arquivo n√£o possui cabe√ßalho.

- Em seguida, as colunas recebem nomes corretos:
  - `cnpj_basico`
  - `opcao_pelo_simples`

 - As colunas estavam sem titulos foi preciso inserir todas seguindo o [Dicion√°rio de Dados do Cadastro Nacional da Pessoa Jur√≠dica](https://www.gov.br/receitafederal/dados/cnpj-metadados.pdf)

### 3. Padroniza√ß√£o do texto
- Uma **UDF (User Defined Function)** chamada `limpar_texto_udf` aplica a fun√ß√£o `limpar_texto` em todas as colunas do tipo **string** do DataFrame.
- Isso garante que todos os textos fiquem:
  - Sem acentos  
  - Em min√∫sculas  
  - Sem aspas  
  - Sem espa√ßos desnecess√°rios 

### 4. Escrita no Delta Lake
- O DataFrame final √© salvo na **camada Bronze** no formato Delta:
  - `mode("overwrite")` ‚Üí sobrescreve os dados antigos.
  - `option("overwriteSchema", "true")` ‚Üí atualiza o esquema se necess√°rio.

### 5. Logs e verifica√ß√£o
- Mensagens de log informam cada etapa do processo:
  - Arquivo lido com sucesso.
  - Colunas renomeadas corretamente.
  - Textos padronizados.
  - Arquivo salvo no caminho definido.
- No final, √© feita a contagem de linhas (`df.count()`) para confirmar a quantidade de registros carregados.

### 6. Configura√ß√£o da DAG

- O arquivo rfb_cnpj foi configurado para que o Airflow funcione corretamente.
- Todas as tasks configuradas seguindo o fluxo de dados da camada landing para a bronze.

---
=======
## 23 Pipeline de Ingest√£o - Regime Tribut√°rio (RFB)

Este script tem como objetivo automatizar o **download, extra√ß√£o, upload para o MinIO** dos arquivos da Receita Federal referentes ao regime tribut√°rio de entidades.

- Os arquivos s√£o:
1. Imunes e Isentas;
2. Lucro Arbitrado;
3. Lucro Presumido;
4. Lucro Real

---

## üìå Etapas do Processo

### 1. Defini√ß√£o de par√¢metros
- Define a **URL base** dos arquivos da Receita Federal.
- Especifica o **bucket** no MinIO onde os arquivos ser√£o armazenados.
- Lista os **arquivos ZIP** que devem ser baixados.

---

### 2. Prepara√ß√£o da pasta de staging
- Cria a pasta `download/`, que serve como √°rea tempor√°ria para armazenar os arquivos baixados.

---

### 3. Fun√ß√£o de download e extra√ß√£o
A fun√ß√£o `download_and_extract`:
1. Faz o **download** do arquivo ZIP com m√∫ltiplas tentativas em caso de falha (at√© 3).
2. Verifica se:
   - O **status da resposta** √© `200 OK`.
   - O **tipo do conte√∫do** √© realmente um `.zip`.
   - O **tamanho do arquivo** √© v√°lido.
3. Salva o arquivo na pasta `download/`.
4. Realiza a **extra√ß√£o** do conte√∫do do ZIP (arquivos `.csv`).
5. Registra logs para acompanhar sucesso ou erros.

---

### 4. Upload para o MinIO
- Localiza todos os arquivos `.csv` extra√≠dos na pasta `download/`.
- Para cada arquivo:
  - Define o **caminho relativo** dentro do bucket MinIO.
  - Faz o **upload** para o bucket `landing`, no caminho:
    ```
    rfb/regime_tributario/imunes_isentas/<arquivo.csv>
    ```

---

### 5. Limpeza p√≥s-processamento
- Ap√≥s o upload, remove a pasta `download/` e todos os arquivos tempor√°rios.
- Caso a pasta n√£o exista ou ocorra erro na remo√ß√£o, o log registra o ocorrido.

---

## ‚öôÔ∏è Principais Bibliotecas Utilizadas
- **requests** ‚Üí Download dos arquivos da Receita Federal.  
- **zipfile** ‚Üí Extra√ß√£o dos arquivos ZIP.  
- **pathlib** ‚Üí Manipula√ß√£o de caminhos de diret√≥rios.  
- **minio** ‚Üí Upload dos arquivos processados para o MinIO.  
- **shutil** ‚Üí Limpeza da pasta tempor√°ria.  
- **logging** ‚Üí Registro de logs para monitoramento do processo.  

---

## üìÇ Fluxo Resumido
1. Baixar arquivo ZIP da Receita Federal.  
2. Validar e extrair os arquivos CSV.  
3. Enviar os arquivos para o MinIO.  
4. Limpar a pasta de staging.  

---

## 24 Processamento dos arquivos da camada landing para bronze ‚Äì Regime Tribut√°rio

O objetivo do c√≥digo foi:

- Criar um notebook para cada task;
- Processar, padronizar as informa√ß√µes e salvar em pastas por tipo, assim como foi feito da camada landing.
- Salvar os dados no Delta Lake da camada bronze.

1. Fun√ß√µes de padroniza√ß√£o de texto

  * padronizar_texto(df: DataFrame)

    * Aplica strip e upper em todas as colunas do tipo string.

    * Remove espa√ßos desnecess√°rios no in√≠cio/fim e transforma todos os textos em min√∫sculos.

    * Garantia de consist√™ncia para joins e compara√ß√µes futuras.

2. Fun√ß√µes de padroniza√ß√£o de n√∫meros e c√≥digos

  * padronizar_numeros_e_codigos(df: DataFrame)

    * Converte colunas inteiras (int, bigint) para int.

    * Converte colunas decimais (double, float) para double.

    * Padroniza CNPJs nas colunas cnpj e cnpj_da_scp:

    * Remove todos os caracteres que n√£o s√£o n√∫meros (\D).

    * Preenche com zeros √† esquerda at√© 14 d√≠gitos (lpad).

    * Garantia de consist√™ncia em identificadores e c√°lculos.

3. Leitura do arquivo CSV

 - Os arquivos CSV s√£o lidos do S3 (MinIO) usando Spark, via s3a://:

 - bucket = landing

- pasta = rfb/regime_tributario/lucro_real/ (exemplo)

- input_path = s3a://landing/rfb/regime_tributario/lucro_real/*.csv

Permite processar m√∫ltiplos arquivos de uma vez, mantendo a consist√™ncia.

4. Aplica√ß√£o das padroniza√ß√µes

- O DataFrame lido passa pelas fun√ß√µes de padroniza√ß√£o:

- padronizar_texto(df) ‚Üí normaliza todos os textos.

- padronizar_numeros_e_codigos(df) ‚Üí normaliza n√∫meros e c√≥digos.

- Resultado: dados limpos, consistentes e prontos para camadas superiores.

5. Escrita no Delta Lake

- O DataFrame final √© salvo na camada bronze no formato Delta:

~~~bash

df.write.format("delta") \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .save(output_path)

~~~

- mode("overwrite") ‚Üí sobrescreve os dados antigos.

- option("overwriteSchema", "true") ‚Üí atualiza o schema caso haja altera√ß√µes.

- output_path organiza os dados por subcategoria e tabela, por exemplo:

~~~bash
s3a://bronze/rfb/regime_tributario/regime_tributario_lucro_real/
~~~

6. Logs e verifica√ß√£o

- O processo registra logs em cada etapa:

    - Arquivo CSV lido do bucket de origem.

    - Colunas e tipos padronizados.

    - Arquivo salvo em Delta Lake no caminho de sa√≠da.

- Ao final, √© feita a contagem de linhas (df.count()) para confirmar o total de registros processados.

7. Configura√ß√£o para DAG (Airflow)

- O notebook/task foi preparado para integra√ß√£o com o Airflow, garantindo:

- Processamento autom√°tico de novos arquivos.

- Fluxo cont√≠nuo da camada landing para bronze.

- Padroniza√ß√£o consistente de todos os arquivos de regime tribut√°rio.

---